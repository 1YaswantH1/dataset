import numpy as np

class MDP:
    def __init__(self, states, actions, transition_prob, rewards, gamma):
        self.states = states
        self.actions = actions
        self.transition_prob = transition_prob  # P(s' | s, a)
        self.rewards = rewards  # R(s, a, s')
        self.gamma = gamma

    def value_iteration(self, epsilon=1e-6):
        V = np.zeros(len(self.states))
        while True:
            delta = 0
            for s in self.states:
                v = V[s]
                V[s] = max(
                    sum(
                        self.transition_prob[s][a][s_next] * 
                        (self.rewards[s][a][s_next] + self.gamma * V[s_next])
                        for s_next in self.states
                    )
                    for a in self.actions
                )
                delta = max(delta, abs(v - V[s]))
            if delta < epsilon:
                break
        return V

    def policy_iteration(self):
        policy = np.zeros(len(self.states), dtype=int)
        V = np.zeros(len(self.states))

        def one_step_lookahead(s, V):
            A = np.zeros(len(self.actions))
            for a in self.actions:
                A[a] = sum(
                    self.transition_prob[s][a][s_next] * 
                    (self.rewards[s][a][s_next] + self.gamma * V[s_next])
                    for s_next in self.states
                )
            return A

        while True:
            # Policy Evaluation
            while True:
                delta = 0
                for s in self.states:
                    v = V[s]
                    V[s] = sum(
                        self.transition_prob[s][policy[s]][s_next] * 
                        (self.rewards[s][policy[s]][s_next] + self.gamma * V[s_next])
                        for s_next in self.states
                    )
                    delta = max(delta, abs(v - V[s]))
                if delta < 1e-6:
                    break

            # Policy Improvement
            policy_stable = True
            for s in self.states:
                old_action = policy[s]
                policy[s] = np.argmax(one_step_lookahead(s, V))
                if old_action != policy[s]:
                    policy_stable = False

            if policy_stable:
                return policy, V

# Example usage
states = [0, 1, 2]
actions = [0, 1]
transition_prob = {
    0: {0: {0: 0.8, 1: 0.2}, 1: {1: 0.9, 2: 0.1}},
    1: {0: {0: 0.7, 2: 0.3}, 1: {0: 0.4, 2: 0.6}},
    2: {0: {1: 0.5, 2: 0.5}, 1: {0: 0.6, 1: 0.4}}
}
rewards = {
    0: {0: {0: 5, 1: 10}, 1: {1: -1, 2: 2}},
    1: {0: {0: -1, 2: 1}, 1: {0: 1, 2: 3}},
    2: {0: {1: -2, 2: 0}, 1: {0: 0, 1: 1}}
}
gamma = 0.9

mdp = MDP(states, actions, transition_prob, rewards, gamma)

# Value Iteration
optimal_values = mdp.value_iteration()
print("Optimal Values from Value Iteration:", optimal_values)

# Policy Iteration
optimal_policy, optimal_values = mdp.policy_iteration()
print("Optimal Policy from Policy Iteration:", optimal_policy)
print("Optimal Values from Policy Iteration:", optimal_values)
